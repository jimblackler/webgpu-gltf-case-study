<!doctype html>

<html>
  <head>
    <meta charset='utf-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1, user-scalable=no'>
    <meta name='mobile-web-app-capable' content='yes'>
    <meta name='apple-mobile-web-app-capable' content='yes'>

    <!-- Valid until June 23, 2022-->
    <meta http-equiv="origin-trial" content="AtHQISnXB5Y+0ffMLL3C/Zvf24IVI+s5kcP3wbueQbRvBKPpfHckADLzhuWAM4/K4aYBZQnSKIBAX4T8ZacOdQ4AAABOeyJvcmlnaW4iOiJodHRwczovL3RvamkuZ2l0aHViLmlvOjQ0MyIsImZlYXR1cmUiOiJXZWJHUFUiLCJleHBpcnkiOjE2NjM3MTgzOTl9">

    <title>01 - Naive glTF renderer</title>
  </head>
  <body>
    <canvas class='webgpu-canvas'></canvas>
    <script type="module">
      import { GltfDemo } from './js/gltf-demo.js'
      import { TinyGltfWebGpu } from './js/tiny-gltf.js'

      // We can map the attributes to any location index we want as long as we're consistent
      // between the pipeline definitions and the shader source.
      const ShaderLocations = {
        POSITION: 0,
        NORMAL: 1,
      };

      // This renderer class will handle the bits of the glTF loading/rendering that we're most
      // interested in. Specifically: Creating the necessary pipelines and bind groups and
      // performing the actuall bind and draw calls during the render loop.
      class GltfRenderer {
        // Not bothering with textures in this sample, so skip loading them.
        static loadImageSlots = [];

        // Associates a glTF node or primitive with its WebGPU resources.
        nodeGpuData = new Map();
        primitiveGpuData = new Map();

        constructor(demoApp, gltf) {
          this.gltf = gltf;
          this.app = demoApp;
          this.device = demoApp.device;

          // We need a bind group layout for the transform uniforms of each node.
          this.nodeBindGroupLayout = this.device.createBindGroupLayout({
            label: `glTF Node BindGroupLayout`,
            entries: [{
              binding: 0, // Node uniforms
              visibility: GPUShaderStage.VERTEX,
              buffer: {},
            }],
          });

          // Everything we'll render with these pages can share a single pipeline layout.
          // A more advanced renderer supporting things like skinning or multiple material types
          // may need more.
          this.gltfPipelineLayout = this.device.createPipelineLayout({
            label: 'glTF Pipeline Layout',
            bindGroupLayouts: [
              this.app.frameBindGroupLayout,
              this.nodeBindGroupLayout,
            ]
          });

          // Find every node with a mesh and create a bind group containing the node's transform.
          for (const node of gltf.nodes) {
            if ('mesh' in node) {
              this.setupMeshNode(gltf, node);
            }
          }

          // Loop through each primitive of each mesh and create a compatible WebGPU pipeline.
          for (const mesh of gltf.meshes) {
            for (const primitive of mesh.primitives) {
              this.setupPrimitive(gltf, primitive);
            }
          }
        }

        getShaderModule() {
          // Cache the shader module, since all the pipelines use the same one.
          if (!this.shaderModule) {
            // The shader source used here is intentionally minimal. It just displays the geometry
            // as white with a very simplistic directional lighting based only on vertex normals
            // (just to show the shape of the mesh a bit better.)
            const code = `
              struct Camera {
                projection : mat4x4f,
                view : mat4x4f,
                position : vec3f,
                time : f32,
              };
              @group(0) @binding(0) var<uniform> camera : Camera;

              struct Model {
                matrix: mat4x4f,
                normalMat: mat4x4f,
              }
              @group(1) @binding(0) var<uniform> model : Model;

              struct VertexInput {
                @location(${ShaderLocations.POSITION}) position : vec3f,
                @location(${ShaderLocations.NORMAL}) normal : vec3f,
              };

              struct VertexOutput {
                @builtin(position) position : vec4f,
                @location(0) normal : vec3f,
              };

              @vertex
              fn vertexMain(input : VertexInput) -> VertexOutput {
                var output : VertexOutput;

                output.position = camera.projection * camera.view * model.matrix * vec4f(input.position, 1);
                output.normal = (camera.view * model.normalMat * vec4f(input.normal, 0)).xyz;

                return output;
              }

              // Some hardcoded lighting
              const lightDir = vec3f(0.25, 0.5, 1);
              const lightColor = vec3f(1);
              const ambientColor = vec3f(0.1);

              @fragment
              fn fragmentMain(input : VertexOutput) -> @location(0) vec4f {
                // An extremely simple directional lighting model, just to give our model some shape.
                let N = normalize(input.normal);
                let L = normalize(lightDir);
                let NDotL = max(dot(N, L), 0.0);
                let surfaceColor = ambientColor + NDotL;

                return vec4f(surfaceColor, 1);
              }
            `;

            this.shaderModule = this.device.createShaderModule({
              label: 'Simple glTF rendering shader module',
              code,
            });
          }

          return this.shaderModule;
        }

        setupMeshNode(gltf, node) {
          // Create a uniform buffer for this node and populate it with the node's world transform
          // and normal matrix (transpose inverse of the world matrix).
          const nodeUniformBuffer = this.device.createBuffer({
            size: 32 * Float32Array.BYTES_PER_ELEMENT,
            usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
          });
          this.device.queue.writeBuffer(nodeUniformBuffer, 0, node.worldMatrix);
          this.device.queue.writeBuffer(nodeUniformBuffer, 16 * Float32Array.BYTES_PER_ELEMENT, node.normalMatrix);

          // Create a bind group containing the uniform buffer for this node.
          const bindGroup = this.device.createBindGroup({
            label: `glTF Node BindGroup`,
            layout: this.nodeBindGroupLayout,
            entries: [{
              binding: 0, // Node uniforms
              resource: { buffer: nodeUniformBuffer },
            }],
          });

          this.nodeGpuData.set(node, { bindGroup });
        }

        setupPrimitive(gltf, primitive) {
          const bufferLayout = [];
          const gpuBuffers = [];
          let drawCount = 0;

          // Loop through every attribute in the primitive and build a description of the vertex
          // layout, which is needed to create the render pipeline.
          for (const [attribName, accessorIndex] of Object.entries(primitive.attributes)) {
            const accessor = gltf.accessors[accessorIndex];
            const bufferView = gltf.bufferViews[accessor.bufferView];

            // Get the shader location for this attribute. If it doesn't have one skip over the
            // attribute because we don't need it for rendering (yet).
            const shaderLocation = ShaderLocations[attribName];
            if (shaderLocation === undefined) { continue; }

            // Create a new vertex buffer entry for the render pipeline that describes this
            // attribute. Implicitly assumes that one buffer will be bound per attribute, even if
            // the attribute data is interleaved.
            bufferLayout.push({
              arrayStride: bufferView.byteStride || TinyGltfWebGpu.packedArrayStrideForAccessor(accessor),
              attributes: [{
                shaderLocation,
                format: TinyGltfWebGpu.gpuFormatForAccessor(accessor),
                offset: accessor.byteOffset,
              }]
            });

            // Since we're skipping some attributes, we need to track the WebGPU buffers that are
            // used here so that we can bind them in the correct order at draw time.
            gpuBuffers.push(gltf.gpuBuffers[accessor.bufferView]);

            // All attributes should have the same count, which will be the draw count for
            // non-indexed geometry.
            drawCount = accessor.count;
          }

          // Create a render pipeline that is compatible with the vertex buffer layout for this
          // primitive. Doesn't yet take into account any material properties.
          const module = this.getShaderModule();
          const pipeline = this.device.createRenderPipeline({
            label: 'glTF renderer pipeline',
            layout: this.gltfPipelineLayout,
            vertex: {
              module,
              entryPoint: 'vertexMain',
              buffers: bufferLayout,
            },
            primitive: {
              topology: TinyGltfWebGpu.gpuPrimitiveTopologyForMode(primitive.mode),
              cullMode: 'back',
            },
            multisample: {
              count: 1,
            },
            depthStencil: {
              format: this.app.depthFormat,
              depthWriteEnabled: true,
              depthCompare: 'less',
            },
            fragment: {
              module,
              entryPoint: 'fragmentMain',
              targets: [{
                format: this.app.colorFormat,
              }],
            },
          });

          // Store data needed to render this primitive.
          const gpuPrimitive = { pipeline, buffers: gpuBuffers, drawCount };

          // If the primitive has index data, store the index buffer, offset, type, count as well.
          if ('indices' in primitive) {
            const accessor = gltf.accessors[primitive.indices];
            gpuPrimitive.indexBuffer = gltf.gpuBuffers[accessor.bufferView];
            gpuPrimitive.indexOffset = accessor.byteOffset;
            gpuPrimitive.indexType = TinyGltfWebGpu.gpuIndexFormatForComponentType(accessor.componentType);
            gpuPrimitive.drawCount = accessor.count;
          }

          this.primitiveGpuData.set(primitive, gpuPrimitive);
        }

        render(renderPass) {
          // Set a bind group with any necessary per-frame uniforms, such as the perspective and
          // view matrices. (This is managed in tiny-webgpu-demo.js)
          renderPass.setBindGroup(0, this.app.frameBindGroup);

          // Loop through all of the nodes that we created transform uniforms for in the
          // constructor and set those bind groups now.
          for (const [node, gpuNode] of this.nodeGpuData) {
            renderPass.setBindGroup(1, gpuNode.bindGroup);

            // Find the mesh for this node and loop through all of its primitives.
            const mesh = this.gltf.meshes[node.mesh];
            for (const primitive of mesh.primitives) {
              const gpuPrimitive = this.primitiveGpuData.get(primitive);

              // Set the pipeline for this primitive.
              renderPass.setPipeline(gpuPrimitive.pipeline);

              // Set the vertex buffers for this primitive.
              for (const [bufferIndex, gpuBuffer] of Object.entries(gpuPrimitive.buffers)) {
                renderPass.setVertexBuffer(bufferIndex, gpuBuffer);
              }

              if (gpuPrimitive.indexBuffer) {
                // If the primitive has indices, set the index buffer and draw indexed geometry.
                renderPass.setIndexBuffer(gpuPrimitive.indexBuffer, gpuPrimitive.indexType, gpuPrimitive.indexOffset);
                renderPass.drawIndexed(gpuPrimitive.drawCount);
              } else {
                // Otherwise draw non-indexed geometry.
                renderPass.draw(gpuPrimitive.drawCount);
              }
            }
          }
        }
      }

      const demo = new GltfDemo(GltfRenderer);
    </script>
  </body>
</html>
